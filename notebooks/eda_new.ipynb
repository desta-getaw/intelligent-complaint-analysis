{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ“Š CFPB Complaints EDA & Preprocessing Notebook\n",
    "\n",
    "This notebook:\n",
    "âœ… Loads the dataset\n",
    "âœ… Analyzes product distribution and narrative lengths\n",
    "âœ… Counts complaints with/without narratives\n",
    "âœ… Filters for specific products & cleans narratives\n",
    "âœ… Saves the final cleaned dataset\n",
    "\n",
    "All functions come from the modular scripts in the `src` folder, which are imported below.\n"
   ],
   "id": "92889a4f54d69549"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ“Š CFPB Complaints EDA & Preprocessing\n",
    "\n",
    "This notebook:\n",
    "âœ… Analyzes distribution of complaints across products\n",
    "âœ… Calculates and visualizes word counts of complaint narratives\n",
    "âœ… Counts how many complaints have / donâ€™t have narratives\n",
    "âœ… Filters dataset for specific products & removes empty narratives\n",
    "âœ… Cleans narrative text for embedding quality\n"
   ],
   "id": "af47e7909a5453c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T23:35:47.640318Z",
     "start_time": "2025-07-05T23:35:44.331420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "%pip install pandas matplotlib seaborn scikit-learn"
   ],
   "id": "ffb3efed1d0ee5d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T23:33:45.351070Z",
     "start_time": "2025-07-05T23:33:45.347628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root directory to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ],
   "id": "26c668616e8ee521",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T23:36:13.597427Z",
     "start_time": "2025-07-05T23:35:53.623285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_raw = pd.read_csv(\"../data/raw/complaints.csv\", usecols=[\"Product\"])\n",
    "print(df_raw['Product'].dropna().str.strip().str.lower().unique())"
   ],
   "id": "2ee3a9775c0be01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['credit reporting or other personal consumer reports' 'debt collection'\n",
      " 'credit card' 'checking or savings account'\n",
      " 'money transfer, virtual currency, or money service'\n",
      " 'vehicle loan or lease' 'debt or credit management' 'mortgage'\n",
      " 'payday loan, title loan, personal loan, or advance loan' 'prepaid card'\n",
      " 'student loan' 'credit reporting'\n",
      " 'credit reporting, credit repair services, or other personal consumer reports'\n",
      " 'credit card or prepaid card' 'payday loan, title loan, or personal loan'\n",
      " 'bank account or service' 'money transfers' 'consumer loan' 'payday loan'\n",
      " 'other financial service' 'virtual currency']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T23:36:52.592207Z",
     "start_time": "2025-07-05T23:36:27.387936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load only the 'Product' column from the CSV file\n",
    "file_path = \"../data/raw/complaints.csv\"\n",
    "columns_to_load = [\"Product\"]\n",
    "\n",
    "df = pd.read_csv(file_path, usecols=columns_to_load)\n",
    "\n",
    "# Clean the 'Product' column: drop missing values, strip spaces, convert to lowercase, get unique values\n",
    "unique_products = (\n",
    "    df['Product']\n",
    "    .dropna()\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "print(unique_products)\n"
   ],
   "id": "6602298380abdadc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['credit reporting or other personal consumer reports' 'debt collection'\n",
      " 'credit card' 'checking or savings account'\n",
      " 'money transfer, virtual currency, or money service'\n",
      " 'vehicle loan or lease' 'debt or credit management' 'mortgage'\n",
      " 'payday loan, title loan, personal loan, or advance loan' 'prepaid card'\n",
      " 'student loan' 'credit reporting'\n",
      " 'credit reporting, credit repair services, or other personal consumer reports'\n",
      " 'credit card or prepaid card' 'payday loan, title loan, or personal loan'\n",
      " 'bank account or service' 'money transfers' 'consumer loan' 'payday loan'\n",
      " 'other financial service' 'virtual currency']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T23:39:05.700661Z",
     "start_time": "2025-07-05T23:39:04.229718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ],
   "id": "3e81c5f4b717faa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\kifiya ai master training program 5 6 &7\\week-5\\credit-risk-model\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\desta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\desta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T04:42:45.684666Z",
     "start_time": "2025-07-06T04:40:09.067122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure src_path is in sys.path\n",
    "from src.preprocessing import run_preprocessing\n",
    "\n",
    "# Note: The dataset might not contain explicit BNPL product rows,\n",
    "# but we still recheck and map based on narrative text.\n",
    "df_clean = run_preprocessing()\n",
    "\n",
    "# Display first few rows\n",
    "print(df_clean.head())\n"
   ],
   "id": "d9a5a670bc37ca83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Starting chunked preprocessing...\n",
      "ðŸ“¦ Processing chunk 1...\n",
      "ðŸ“¦ Processing chunk 2...\n",
      "ðŸ“¦ Processing chunk 3...\n",
      "ðŸ“¦ Processing chunk 4...\n",
      "ðŸ“¦ Processing chunk 5...\n",
      "ðŸ“¦ Processing chunk 6...\n",
      "ðŸ“¦ Processing chunk 7...\n",
      "ðŸ“¦ Processing chunk 8...\n",
      "ðŸ“¦ Processing chunk 9...\n",
      "ðŸ“¦ Processing chunk 10...\n",
      "ðŸ“¦ Processing chunk 11...\n",
      "ðŸ“¦ Processing chunk 12...\n",
      "ðŸ“¦ Processing chunk 13...\n",
      "ðŸ“¦ Processing chunk 14...\n",
      "ðŸ“¦ Processing chunk 15...\n",
      "ðŸ“¦ Processing chunk 16...\n",
      "ðŸ“¦ Processing chunk 17...\n",
      "ðŸ“¦ Processing chunk 18...\n",
      "ðŸ“¦ Processing chunk 19...\n",
      "ðŸ“¦ Processing chunk 20...\n",
      "ðŸ“¦ Processing chunk 21...\n",
      "ðŸ“¦ Processing chunk 22...\n",
      "ðŸ“¦ Processing chunk 23...\n",
      "ðŸ“¦ Processing chunk 24...\n",
      "ðŸ“¦ Processing chunk 25...\n",
      "ðŸ“¦ Processing chunk 26...\n",
      "ðŸ“¦ Processing chunk 27...\n",
      "ðŸ“¦ Processing chunk 28...\n",
      "ðŸ“¦ Processing chunk 29...\n",
      "ðŸ“¦ Processing chunk 30...\n",
      "ðŸ“¦ Processing chunk 31...\n",
      "ðŸ“¦ Processing chunk 32...\n",
      "ðŸ“¦ Processing chunk 33...\n",
      "ðŸ“¦ Processing chunk 34...\n",
      "ðŸ“¦ Processing chunk 35...\n",
      "ðŸ“¦ Processing chunk 36...\n",
      "ðŸ“¦ Processing chunk 37...\n",
      "ðŸ“¦ Processing chunk 38...\n",
      "ðŸ“¦ Processing chunk 39...\n",
      "ðŸ“¦ Processing chunk 40...\n",
      "ðŸ“¦ Processing chunk 41...\n",
      "ðŸ“¦ Processing chunk 42...\n",
      "ðŸ“¦ Processing chunk 43...\n",
      "ðŸ“¦ Processing chunk 44...\n",
      "ðŸ“¦ Processing chunk 45...\n",
      "ðŸ“¦ Processing chunk 46...\n",
      "ðŸ“¦ Processing chunk 47...\n",
      "ðŸ“¦ Processing chunk 48...\n",
      "ðŸ“¦ Processing chunk 49...\n",
      "ðŸ“¦ Processing chunk 50...\n",
      "ðŸ“¦ Processing chunk 51...\n",
      "ðŸ“¦ Processing chunk 52...\n",
      "ðŸ“¦ Processing chunk 53...\n",
      "ðŸ“¦ Processing chunk 54...\n",
      "ðŸ“¦ Processing chunk 55...\n",
      "ðŸ“¦ Processing chunk 56...\n",
      "ðŸ“¦ Processing chunk 57...\n",
      "ðŸ“¦ Processing chunk 58...\n",
      "ðŸ“¦ Processing chunk 59...\n",
      "ðŸ“¦ Processing chunk 60...\n",
      "ðŸ“¦ Processing chunk 61...\n",
      "ðŸ“¦ Processing chunk 62...\n",
      "ðŸ“¦ Processing chunk 63...\n",
      "ðŸ“¦ Processing chunk 64...\n",
      "ðŸ“¦ Processing chunk 65...\n",
      "ðŸ“¦ Processing chunk 66...\n",
      "ðŸ“¦ Processing chunk 67...\n",
      "ðŸ“¦ Processing chunk 68...\n",
      "ðŸ“¦ Processing chunk 69...\n",
      "ðŸ“¦ Processing chunk 70...\n",
      "ðŸ“¦ Processing chunk 71...\n",
      "ðŸ“¦ Processing chunk 72...\n",
      "ðŸ“¦ Processing chunk 73...\n",
      "ðŸ“¦ Processing chunk 74...\n",
      "ðŸ“¦ Processing chunk 75...\n",
      "ðŸ“¦ Processing chunk 76...\n",
      "ðŸ“¦ Processing chunk 77...\n",
      "ðŸ“¦ Processing chunk 78...\n",
      "ðŸ“¦ Processing chunk 79...\n",
      "ðŸ“¦ Processing chunk 80...\n",
      "ðŸ“¦ Processing chunk 81...\n",
      "ðŸ“¦ Processing chunk 82...\n",
      "ðŸ“¦ Processing chunk 83...\n",
      "ðŸ“¦ Processing chunk 84...\n",
      "ðŸ“¦ Processing chunk 85...\n",
      "ðŸ“¦ Processing chunk 86...\n",
      "ðŸ“¦ Processing chunk 87...\n",
      "ðŸ“¦ Processing chunk 88...\n",
      "ðŸ“¦ Processing chunk 89...\n",
      "ðŸ“¦ Processing chunk 90...\n",
      "ðŸ“¦ Processing chunk 91...\n",
      "ðŸ“¦ Processing chunk 92...\n",
      "ðŸ“¦ Processing chunk 93...\n",
      "ðŸ“¦ Processing chunk 94...\n",
      "ðŸ“¦ Processing chunk 95...\n",
      "ðŸ“¦ Processing chunk 96...\n",
      "ðŸ“¦ Processing chunk 97...\n",
      "\n",
      "âœ… EDA Summary\n",
      "Total complaints processed: 9,609,797\n",
      "Complaints with narratives: 2,980,756\n",
      "Complaints without narratives: 6,629,041\n",
      "Complaints retained after filtering: 518,654\n",
      "\n",
      "ðŸ“Š Product distribution:\n",
      "product\n",
      "Credit card           194047\n",
      "Savings account       152106\n",
      "Money transfer         97319\n",
      "Personal loan          75151\n",
      "Buy Now, Pay Later        31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ’¾ Saved filtered dataset to: ../data/filtered/filtered_complaints.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'narrative_len'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Kifiya AI Master Training Program 5 6 &7\\week-5\\credit-risk-model\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3811\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'narrative_len'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msrc\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m run_preprocessing\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Note: The dataset might not contain explicit BNPL product rows,\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# but we still recheck and map based on narrative text.\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m df_clean = \u001B[43mrun_preprocessing\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Display first few rows\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(df_clean.head())\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Kifiya AI Master Training Program 5 6 &7\\week-6\\Intelligent-Complaint-Analysis-for-Financial-Services\\src\\preprocessing.py:129\u001B[39m, in \u001B[36mrun_preprocessing\u001B[39m\u001B[34m(input_path, output_path, plot_dir, chunksize)\u001B[39m\n\u001B[32m    127\u001B[39m \u001B[38;5;66;03m# Plot 1: Narrative length distribution\u001B[39;00m\n\u001B[32m    128\u001B[39m plt.figure(figsize=(\u001B[32m10\u001B[39m, \u001B[32m5\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m129\u001B[39m sns.histplot(\u001B[43mdf_final\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mnarrative_len\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m, bins=\u001B[32m50\u001B[39m, kde=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    130\u001B[39m plt.title(\u001B[33m\"\u001B[39m\u001B[33mDistribution of Narrative Length (Words)\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    131\u001B[39m plt.xlabel(\u001B[33m\"\u001B[39m\u001B[33mNumber of Words\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Kifiya AI Master Training Program 5 6 &7\\week-5\\credit-risk-model\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4105\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4106\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4107\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4108\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4109\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Kifiya AI Master Training Program 5 6 &7\\week-5\\credit-risk-model\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3814\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3815\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3816\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3817\u001B[39m     ):\n\u001B[32m   3818\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3819\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3820\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3821\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3822\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3823\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3824\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'narrative_len'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "efb42f339290a92d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T23:34:23.805144Z",
     "start_time": "2025-07-05T23:34:23.802541Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cf91f7cfd7eada3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a5e0222335ad2189"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "âœ… **Done!**\n",
    "- Dataset filtered to include only target products & non-empty narratives\n",
    "- Narratives cleaned and ready for embedding\n",
    "- File saved to `data/filtered_complaints.csv`\n"
   ],
   "id": "c589b554d76b168d"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
